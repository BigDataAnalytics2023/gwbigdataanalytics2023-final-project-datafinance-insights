{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "043cadf4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL\n",
      "AMZN\n",
      "FB\n",
      "Unable to Format\n",
      "GOOG\n",
      "IBM\n",
      "INTC\n",
      "MSFT\n",
      "SPX\n",
      "Unable to Format\n",
      "TSLA\n",
      "V\n",
      "         date    open     high      low   close    volume stock\n",
      "0  2023-07-25  193.33  194.440  192.915  193.62  37283201  AAPL\n",
      "1  2023-07-26  193.67  195.640  193.320  194.50  47471868  AAPL\n",
      "2  2023-07-27  196.02  197.200  192.550  193.22  47460180  AAPL\n",
      "3  2023-07-28  194.67  196.626  194.140  195.83  48291443  AAPL\n",
      "4  2023-07-31  196.06  196.490  195.260  196.45  38824113  AAPL\n",
      "..        ...     ...      ...      ...     ...       ...   ...\n",
      "95 2023-12-07  254.89  256.140  253.500  255.82   3589256     V\n",
      "96 2023-12-08  255.00  256.040  253.870  255.74   3732515     V\n",
      "97 2023-12-11  255.00  257.630  255.000  256.52   6479312     V\n",
      "98 2023-12-12  257.30  259.720  256.395  259.56   5946564     V\n",
      "99 2023-12-13  259.15  262.480  258.690  262.38   4566474     V\n",
      "\n",
      "[800 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage, bigquery\n",
    "import pandas as pd\n",
    "from google.oauth2 import service_account\n",
    "import json\n",
    "\n",
    "# Set up Google Cloud credentials\n",
    "project_credentials = service_account.Credentials.from_service_account_file('data-finance-final-92d8049c252f.json')\n",
    "project_id = 'data-finance-final'\n",
    "\n",
    "# Initialize the client\n",
    "storage_client = storage.Client(credentials=project_credentials, project=project_id)\n",
    "bucket_name = 'data_finance_final'\n",
    "\n",
    "# Get the bucket\n",
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "# List all blobs in the bucket\n",
    "blobs = list(bucket.list_blobs())\n",
    "\n",
    "# Process the blob names to find the most recent file for each stock\n",
    "latest_files = {}\n",
    "for blob in blobs:\n",
    "    # Assuming file names are in 'Price_STOCK-DATE.json' format\n",
    "    parts = blob.name.split('_')\n",
    "    #print(parts)\n",
    "    if len(parts) > 1 and parts[0] == 'price/Price':\n",
    "        # Extract the stock symbol and date\n",
    "        stock, date_str = parts[1], parts[2].split('.')[0]\n",
    "        date = pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "        \n",
    "        # Determine if this file is the most recent for the stock\n",
    "        if stock not in latest_files or date > latest_files[stock]['date']:\n",
    "            latest_files[stock] = {'date': date, 'blob': blob}\n",
    "\n",
    "# Initialize BigQuery client\n",
    "bigquery_client = bigquery.Client(credentials=project_credentials, project=project_id)\n",
    "dataset_id = 'stockMetaData'\n",
    "table_id = 'stock_prices_cleaned'\n",
    "\n",
    "# Define the schema of your BigQuery table\n",
    "schema = [\n",
    "    bigquery.SchemaField('date', 'DATE'),\n",
    "    bigquery.SchemaField('open', 'FLOAT'),\n",
    "    bigquery.SchemaField('high', 'FLOAT'),\n",
    "    bigquery.SchemaField('low', 'FLOAT'),\n",
    "    bigquery.SchemaField('close', 'FLOAT'),\n",
    "    bigquery.SchemaField('volume', 'INTEGER'),\n",
    "    bigquery.SchemaField('stock', 'String')\n",
    "\n",
    "]\n",
    "\n",
    "# Create or get the dataset and table\n",
    "dataset_ref = bigquery_client.dataset(dataset_id)\n",
    "dataset = bigquery.Dataset(dataset_ref)\n",
    "bigquery_client.create_dataset(dataset, exists_ok=True)\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "table = bigquery.Table(table_ref, schema=schema)\n",
    "bigquery_client.create_table(table, exists_ok=True)\n",
    "\n",
    "df_combined = pd.DataFrame()\n",
    "# For each stock, load the most recent data into BigQuery\n",
    "for stock, file_info in latest_files.items():\n",
    "    print(stock)\n",
    "    \n",
    "    try:\n",
    "        # Download the blob to a local variable\n",
    "        data_string = file_info['blob'].download_as_string()\n",
    "        data_json = json.loads(data_string)\n",
    "\n",
    "        # Process and clean the data as needed, then convert to DataFrame\n",
    "        # This is an example, you'll need to adjust it to match your JSON structure\n",
    "        df = pd.DataFrame.from_dict(data_json['Time Series (Daily)'], orient='index')\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df.sort_index(inplace=True)\n",
    "\n",
    "        # Reset the index and rename columns\n",
    "        df.reset_index(inplace=True)\n",
    "        df.rename(columns={'index': 'date'}, inplace=True)\n",
    "        df.columns = ['date', 'open', 'high', 'low', 'close', 'volume']\n",
    "        df['stock'] = stock\n",
    "\n",
    "        df['open'] = pd.to_numeric(df['open'], errors='coerce')\n",
    "        df['high'] = pd.to_numeric(df['high'], errors='coerce')\n",
    "        df['low'] = pd.to_numeric(df['low'], errors='coerce')\n",
    "        df['close'] = pd.to_numeric(df['close'], errors='coerce')\n",
    "        df['volume'] = pd.to_numeric(df['volume'], downcast='integer', errors='coerce')\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        #Check for and handle any possible duplicates in the 'date' column\n",
    "        df = df.drop_duplicates(subset='date')\n",
    "        df = df.sort_values(by='date')\n",
    "        \n",
    "        df_combined = pd.concat([df_combined,df], axis=0)\n",
    "    except KeyError as e:\n",
    "        print(\"Unable to Format\")\n",
    "\n",
    "print(df_combined)\n",
    "try:\n",
    "    # If the table doesn't exist, create it, otherwise append the data\n",
    "    df_combined.to_gbq(destination_table=f'{dataset_id}.{table_id}', project_id=project_id, if_exists='replace', credentials=project_credentials)\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"Unable to Format\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487ffc1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
